#!/usr/bin/env python3
"""
–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ zakupki.gov.ru.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

import requests
from bs4 import BeautifulSoup
import warnings

warnings.filterwarnings('ignore')
try:
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
except:
    pass


def investigate_tender_page(tender_url):
    """–î–µ—Ç–∞–ª—å–Ω–æ –∏—Å—Å–ª–µ–¥—É–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç–µ–Ω–¥–µ—Ä–∞."""

    full_url = f"https://zakupki.gov.ru{tender_url}"

    print(f"\n{'='*70}")
    print(f"–ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –¢–ï–ù–î–ï–†–ê")
    print(f"{'='*70}")
    print(f"URL: {full_url}\n")

    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })

    try:
        response = session.get(full_url, timeout=30, verify=False)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∞–π–ª—ã
        print("üìÑ –í–°–ï –°–°–´–õ–ö–ò –ù–ê –§–ê–ô–õ–´:\n")

        all_links = soup.find_all('a', href=True)
        doc_links = []

        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)

            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            if any(ext in href.lower() for ext in ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar']):
                doc_links.append({
                    'href': href,
                    'text': text
                })
                print(f"   ‚úÖ {text}")
                print(f"      URL: {href[:80]}...")
                print()

        if not doc_links:
            print("   ‚ùå –§–∞–π–ª—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —á–µ—Ä–µ–∑ –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏")

        print(f"\n{'‚îÄ'*70}")
        print("üîç –ü–û–ò–°–ö –°–ï–ö–¶–ò–ò '–î–û–ö–£–ú–ï–ù–¢–´':\n")

        # –ò—â–µ–º —Å–µ–∫—Ü–∏–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        doc_sections = soup.find_all(['div', 'section', 'table'],
                                     class_=lambda x: x and 'doc' in x.lower())

        for i, section in enumerate(doc_sections[:5], 1):
            print(f"–°–µ–∫—Ü–∏—è {i}:")
            print(f"   –ö–ª–∞—Å—Å: {section.get('class')}")
            print(f"   –¢–µ–∫—Å—Ç: {section.get_text()[:100]}...")
            print()

        print(f"\n{'‚îÄ'*70}")
        print("üîç –ü–û–ò–°–ö –ü–û –ö–õ–Æ–ß–ï–í–´–ú –°–õ–û–í–ê–ú:\n")

        keywords = ['–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è', '–¥–æ–∫—É–º–µ–Ω—Ç—ã', '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ', '–ü—Ä–æ–µ–∫—Ç –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞', '–ò–∑–≤–µ—â–µ–Ω–∏–µ']

        for keyword in keywords:
            elements = soup.find_all(text=lambda t: t and keyword in t)
            if elements:
                print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ '{keyword}': {len(elements)} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                for elem in elements[:2]:
                    parent = elem.parent
                    if parent:
                        print(f"      –ö–æ–Ω—Ç–µ–∫—Å—Ç: {str(parent)[:150]}...")
            else:
                print(f"   ‚ùå '{keyword}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

        print(f"\n{'‚îÄ'*70}")
        print("üîç IFRAME –ò –í–õ–û–ñ–ï–ù–ù–´–ï –°–¢–†–ê–ù–ò–¶–´:\n")

        iframes = soup.find_all('iframe')
        if iframes:
            print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(iframes)} iframe(s)")
            for i, iframe in enumerate(iframes, 1):
                src = iframe.get('src', '')
                print(f"   {i}. {src}")
        else:
            print("   ‚ùå iframe –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

        print(f"\n{'‚îÄ'*70}")
        print("üìã –¢–ê–ë–õ–ò–¶–´ –° –î–ê–ù–ù–´–ú–ò:\n")

        tables = soup.find_all('table')
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Ç–∞–±–ª–∏—Ü: {len(tables)}")

        for i, table in enumerate(tables[:3], 1):
            print(f"\n–¢–∞–±–ª–∏—Ü–∞ {i}:")
            rows = table.find_all('tr')
            print(f"   –°—Ç—Ä–æ–∫: {len(rows)}")
            if rows:
                for row in rows[:3]:
                    cells = row.find_all(['td', 'th'])
                    if cells:
                        text = ' | '.join(c.get_text(strip=True) for c in cells)
                        print(f"      {text[:80]}...")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è
        debug_file = Path(__file__).parent / 'output' / 'debug_tender_page.html'
        debug_file.parent.mkdir(parents=True, exist_ok=True)

        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(soup.prettify())

        print(f"\n{'='*70}")
        print(f"üíæ –ü–æ–ª–Ω—ã–π HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {debug_file}")
        print(f"{'='*70}\n")

        return doc_links

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return []


def main():
    print("\n" + "="*70)
    print("  –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –î–û–°–¢–£–ü–ê –ö –î–û–ö–£–ú–ï–ù–¢–ê–ú")
    print("="*70 + "\n")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–µ–Ω–¥–µ—Ä–æ–≤
    test_tenders = [
        "/epz/order/notice/zk20/view/common-info.html?regNumber=0322200027425000278",
        "/epz/order/notice/ea20/view/common-info.html?regNumber=0352100025025000104",
    ]

    for tender_url in test_tenders:
        doc_links = investigate_tender_page(tender_url)

        if doc_links:
            print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(doc_links)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        else:
            print(f"\n‚ö†Ô∏è  –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∏–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è")

        print("\n" + "="*70 + "\n")

        # –ò—Å—Å–ª–µ–¥—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π —Ç–µ–Ω–¥–µ—Ä –¥–ª—è –Ω–∞—á–∞–ª–∞
        break


if __name__ == "__main__":
    main()
