"""
–ú–æ–¥—É–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–µ–Ω–¥–µ—Ä–æ–≤ zakupki.gov.ru.
"""

from typing import List, Dict, Any, Optional
import requests
from bs4 import BeautifulSoup
import re
from pathlib import Path
import time
import warnings
import os
import zipfile

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL
warnings.filterwarnings('ignore')
try:
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
except:
    pass


class ZakupkiDocumentDownloader:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–µ–Ω–¥–µ—Ä–æ–≤ zakupki.gov.ru.
    """

    def __init__(self, download_dir: Optional[Path] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞.

        Args:
            download_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        self.download_dir = download_dir or Path.cwd() / 'downloaded_tenders'
        self.download_dir.mkdir(parents=True, exist_ok=True)

        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive'
        })

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–∫—Å–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        proxy_url = os.getenv('PROXY_URL', '').strip()
        if proxy_url:
            self.session.proxies = {
                'http': proxy_url,
                'https': proxy_url
            }
            print(f"üîê Document downloader –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–∫—Å–∏: {proxy_url.split('@')[-1] if '@' in proxy_url else proxy_url}")

        # –¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç
        self.document_types = {
            'tech_spec': ['—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è', '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ', '—Ç–∑'],
            'contract': ['–ø—Ä–æ–µ–∫—Ç –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞', '–ø—Ä–æ–µ–∫—Ç –¥–æ–≥–æ–≤–æ—Ä–∞', '–¥–æ–≥–æ–≤–æ—Ä'],
            'notice': ['–∏–∑–≤–µ—â–µ–Ω–∏–µ', '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è'],
            'specification': ['—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è', '—Å–º–µ—Ç–∞'],
        }

    def get_tender_documents(self, tender_url: str, tender_number: str) -> List[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–µ–Ω–¥–µ—Ä–∞.

        Args:
            tender_url: URL –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–µ–Ω–¥–µ—Ä–∞ (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –ø–æ–ª–Ω—ã–π)
            tender_number: –ù–æ–º–µ—Ä —Ç–µ–Ω–¥–µ—Ä–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞–ø–∫–∏

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
        """
        # –§–æ—Ä–º–∏—Ä—É–µ–º URL –≤–∫–ª–∞–¥–∫–∏ "–î–æ–∫—É–º–µ–Ω—Ç—ã"
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç–µ–Ω–¥–µ—Ä–∞ –∏–∑ URL
        tender_type = None
        if '/notice/zk20/' in tender_url or '/notice/zk20/' in tender_url:
            tender_type = 'zk20'
        elif '/notice/ea20/' in tender_url or '/notice/ea44/' in tender_url:
            tender_type = 'ea20'
        elif '/notice/ok20/' in tender_url or '/notice/ok44/' in tender_url:
            tender_type = 'ok20'
        else:
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ URL
            match = re.search(r'/notice/([^/]+)/', tender_url)
            if match:
                tender_type = match.group(1)

        if not tender_type:
            print(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Ç–µ–Ω–¥–µ—Ä–∞ –∏–∑ URL")
            tender_type = 'zk20'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é

        # –§–æ—Ä–º–∏—Ä—É–µ–º URL –≤–∫–ª–∞–¥–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        docs_url = f'https://zakupki.gov.ru/epz/order/notice/{tender_type}/view/documents.html?regNumber={tender_number}'

        print(f"\nüìÑ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ç–µ–Ω–¥–µ—Ä–∞ {tender_number}...")
        print(f"   üîó URL –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {docs_url}")

        # Retry –ª–æ–≥–∏–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ Connection reset
        max_retries = 3
        retry_delay = 2

        for attempt in range(max_retries):
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
                response = self.session.get(docs_url, timeout=30, verify=False)
                response.raise_for_status()
                break  # –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∑–∏–ª–∏, –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞

            except (requests.exceptions.ConnectionError, ConnectionResetError) as e:
                if attempt < max_retries - 1:
                    print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}): {type(e).__name__}")
                    print(f"   ‚è≥ –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {retry_delay} —Å–µ–∫...")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                else:
                    raise  # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    print(f"   ‚è±Ô∏è  –¢–∞–π–º–∞—É—Ç (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries})")
                    print(f"   ‚è≥ –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {retry_delay} —Å–µ–∫...")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    raise

        try:

            soup = BeautifulSoup(response.content, 'html.parser')

            # –ò—â–µ–º —Ä–∞–∑–¥–µ–ª —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            documents = []

            # –ú–µ—Ç–æ–¥ 1: –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å –∫–ª–∞—Å—Å–æ–º 'attachment' (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –¥–ª—è zakupki.gov.ru)
            doc_containers = soup.find_all('div', class_='attachment')

            print(f"   üì¶ –ù–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(doc_containers)}")

            for container in doc_containers:
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                doc_link = container.find('a', href=re.compile(r'44fz/filestore|223fz/filestore'))

                if doc_link:
                    href = doc_link.get('href', '')
                    text = doc_link.get_text(strip=True)

                    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                    if href.startswith('//'):
                        doc_url = f'https:{href}'
                    elif href.startswith('/'):
                        doc_url = f'https://zakupki.gov.ru{href}'
                    elif not href.startswith('http'):
                        doc_url = f'https://zakupki.gov.ru/{href}'
                    else:
                        doc_url = href

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    doc_type = self._classify_document(text, href)

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
                    filename = self._extract_filename_from_text(text, doc_type)

                    documents.append({
                        'url': doc_url,
                        'filename': filename,
                        'title': text or filename,
                        'type': doc_type,
                        'extension': self._get_extension(filename)
                    })

            # –ú–µ—Ç–æ–¥ 2: –ò—â–µ–º –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∞–π–ª—ã (–∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç)
            if not documents:
                print(f"   üîÑ –ò—â–µ–º –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∞–π–ª—ã...")
                doc_links = soup.find_all('a', href=re.compile(r'\.(pdf|doc|docx|xls|xlsx|zip|rar|7z|rtf)$', re.IGNORECASE))

                seen_urls = set()

                for link in doc_links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                    if href in seen_urls:
                        continue

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–æ–∫—É–º–µ–Ω—Ç
                    if not self._is_document_link(href):
                        continue

                    seen_urls.add(href)

                    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                    if href.startswith('//'):
                        doc_url = f'https:{href}'
                    elif href.startswith('/'):
                        doc_url = f'https://zakupki.gov.ru{href}'
                    elif not href.startswith('http'):
                        doc_url = f'https://zakupki.gov.ru/{href}'
                    else:
                        doc_url = href

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    doc_type = self._classify_document(text, href)

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞
                    filename = self._extract_filename(doc_url, text)

                    documents.append({
                        'url': doc_url,
                        'filename': filename,
                        'title': text or filename,
                        'type': doc_type,
                        'extension': self._get_extension(filename)
                    })

            print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")

            # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
            for i, doc in enumerate(documents, 1):
                print(f"      {i}. [{doc['type']}] {doc['title'][:60]} ({doc['extension']})")

            return documents

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []

    def download_documents(
        self,
        tender_url: str,
        tender_number: str,
        doc_types: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–µ–Ω–¥–µ—Ä–∞.

        Args:
            tender_url: URL –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–µ–Ω–¥–µ—Ä–∞
            tender_number: –ù–æ–º–µ—Ä —Ç–µ–Ω–¥–µ—Ä–∞
            doc_types: –¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è (None = –≤—Å–µ)

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∑–∞–≥—Ä—É–∑–∫–∏
        """
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        documents = self.get_tender_documents(tender_url, tender_number)

        if not documents:
            return {
                'tender_number': tender_number,
                'total_documents': 0,
                'downloaded': 0,
                'failed': 0,
                'files': []
            }

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
        if doc_types:
            documents = [doc for doc in documents if doc['type'] in doc_types]

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞
        tender_dir = self.download_dir / self._sanitize_filename(tender_number)
        tender_dir.mkdir(parents=True, exist_ok=True)

        print(f"\nüíæ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤: {tender_dir}")

        # –°–∫–∞—á–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        downloaded_files = []
        failed = 0

        for i, doc in enumerate(documents, 1):
            print(f"   [{i}/{len(documents)}] –°–∫–∞—á–∏–≤–∞–Ω–∏–µ: {doc['title'][:50]}...")

            try:
                # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª
                response = self.session.get(doc['url'], timeout=60, verify=False)
                response.raise_for_status()

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª (—Å–Ω–∞—á–∞–ª–∞ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –∏–º–µ–Ω–µ–º)
                file_path = tender_dir / doc['filename']

                with open(file_path, 'wb') as f:
                    f.write(response.content)

                file_size = len(response.content) / 1024  # KB

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞ –ø–æ magic bytes
                real_type = self._detect_file_type(file_path)

                # –ï—Å–ª–∏ —Ä–µ–∞–ª—å–Ω—ã–π —Ç–∏–ø –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞, –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º
                current_ext = file_path.suffix.lower()
                if real_type and real_type != current_ext.lstrip('.'):
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º
                    new_filename = file_path.stem + '.' + real_type
                    new_file_path = tender_dir / new_filename

                    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Ñ–∞–π–ª
                    file_path.rename(new_file_path)
                    file_path = new_file_path

                    print(f"      üîÑ –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω: .{current_ext} ‚Üí .{real_type}")

                downloaded_files.append({
                    'filename': file_path.name,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è
                    'path': str(file_path),
                    'title': doc['title'],
                    'type': doc['type'],
                    'size_kb': round(file_size, 2),
                    'real_type': real_type  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–π —Ç–∏–ø
                })

                print(f"      ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {file_path.name} ({file_size:.1f} KB)")

                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(0.5)

            except Exception as e:
                print(f"      ‚ùå –û—à–∏–±–∫–∞: {e}")
                failed += 1

        print(f"\n‚úÖ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
        print(f"   üì• –°–∫–∞—á–∞–Ω–æ: {len(downloaded_files)}")
        print(f"   ‚ùå –û—à–∏–±–æ–∫: {failed}")

        return {
            'tender_number': tender_number,
            'tender_dir': str(tender_dir),
            'total_documents': len(documents),
            'downloaded': len(downloaded_files),
            'failed': failed,
            'files': downloaded_files
        }

    def _is_document_link(self, href: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º."""
        doc_extensions = ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.7z', '.rtf']
        href_lower = href.lower()
        return any(href_lower.endswith(ext) for ext in doc_extensions)

    def _classify_document(self, text: str, url: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é."""
        text_lower = text.lower()
        url_lower = url.lower()
        combined = f"{text_lower} {url_lower}"

        for doc_type, keywords in self.document_types.items():
            if any(keyword in combined for keyword in keywords):
                return doc_type

        return 'other'

    def _extract_filename(self, url: str, title: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è."""
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ URL
        from urllib.parse import urlparse, unquote

        parsed = urlparse(url)
        path = unquote(parsed.path)

        if path:
            filename = Path(path).name
            if filename and '.' in filename:
                return self._sanitize_filename(filename)

        # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, —Å–æ–∑–¥–∞–µ–º –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
        if title:
            # –£–±–∏—Ä–∞–µ–º –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã
            clean_title = re.sub(r'[^\w\s\-.]', '', title)
            clean_title = re.sub(r'\s+', '_', clean_title)

            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if '.' not in clean_title:
                # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–∑ URL
                ext = self._get_extension(url)
                clean_title = f"{clean_title}.{ext}"

            return clean_title[:100]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É

        # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è
        return f"document_{int(time.time())}.pdf"

    def _get_extension(self, filename: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞."""
        match = re.search(r'\.([a-z0-9]+)$', filename.lower())
        return match.group(1) if match else 'unknown'

    def _sanitize_filename(self, filename: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤."""
        # –£–±–∏—Ä–∞–µ–º –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è Windows/Unix
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è
        filename = re.sub(r'_+', '_', filename)
        return filename.strip('_')

    def _detect_file_type(self, file_path: Path) -> Optional[str]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞ –ø–æ magic bytes.

        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É

        Returns:
            –¢–∏–ø —Ñ–∞–π–ª–∞: 'pdf', 'docx', 'doc', 'xlsx' –∏–ª–∏ None
        """
        try:
            with open(file_path, 'rb') as f:
                magic = f.read(8)

            # ZIP-based —Ñ–æ—Ä–º–∞—Ç—ã (DOCX, XLSX) –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å PK (50 4B)
            if magic[:2] == b'PK':
                try:
                    with zipfile.ZipFile(file_path, 'r') as zip_ref:
                        namelist = zip_ref.namelist()
                        # DOCX —Å–æ–¥–µ—Ä–∂–∏—Ç word/document.xml
                        if any('word/' in name for name in namelist):
                            return 'docx'
                        # XLSX —Å–æ–¥–µ—Ä–∂–∏—Ç xl/
                        elif any('xl/' in name for name in namelist):
                            return 'xlsx'
                except:
                    pass

            # PDF –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å %PDF (25 50 44 46)
            elif magic[:4] == b'%PDF':
                return 'pdf'

            # –°—Ç–∞—Ä—ã–µ DOC —Ñ–∞–π–ª—ã (OLE Compound Document) –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å D0 CF 11 E0
            elif magic[:4] == b'\xD0\xCF\x11\xE0':
                return 'doc'

            return None

        except Exception as e:
            print(f"      ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞: {e}")
            return None

    def _extract_filename_from_text(self, text: str, doc_type: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        clean_text = re.sub(r'[^\w\s\-.]', '', text)
        clean_text = re.sub(r'\s+', '_', clean_text)
        clean_text = clean_text[:80]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ —Ç–∏–ø—É
        ext_map = {
            'tech_spec': '.pdf',
            'contract': '.pdf',
            'notice': '.pdf',
            'specification': '.pdf',
            'other': '.pdf'
        }

        ext = ext_map.get(doc_type, '.pdf')

        if not clean_text.endswith(ext):
            clean_text = f"{clean_text}{ext}"

        return self._sanitize_filename(clean_text)


def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è."""
    print("\n" + "="*70)
    print("  –¢–ï–°–¢ –ó–ê–ì–†–£–ó–ß–ò–ö–ê –î–û–ö–£–ú–ï–ù–¢–û–í")
    print("="*70 + "\n")

    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫
    downloader = ZakupkiDocumentDownloader()

    # –ü—Ä–∏–º–µ—Ä: —Å–∫–∞—á–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —Ç–µ–Ω–¥–µ—Ä–∞
    # –ù—É–∂–µ–Ω —Ä–µ–∞–ª—å–Ω—ã–π URL —Ç–µ–Ω–¥–µ—Ä–∞ –¥–ª—è —Ç–µ—Å—Ç–∞
    test_tender_url = "/epz/order/notice/ea44/view/common-info.html?regNumber=0173100002023000123"
    test_tender_number = "0173100002023000123"

    print("‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: –î–ª—è —Ç–µ—Å—Ç–∞ –Ω—É–∂–µ–Ω —Ä–µ–∞–ª—å–Ω—ã–π URL —Ç–µ–Ω–¥–µ—Ä–∞ —Å zakupki.gov.ru")
    print(f"–ü—Ä–∏–º–µ—Ä URL: {test_tender_url}\n")

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–±–µ–∑ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è)
    documents = downloader.get_tender_documents(
        f"https://zakupki.gov.ru{test_tender_url}",
        test_tender_number
    )

    if documents:
        print("\n" + "="*70)
        print("  –ù–ê–ô–î–ï–ù–ù–´–ï –î–û–ö–£–ú–ï–ù–¢–´")
        print("="*70 + "\n")

        for i, doc in enumerate(documents, 1):
            print(f"{i}. {doc['title']}")
            print(f"   –¢–∏–ø: {doc['type']}")
            print(f"   –§–∞–π–ª: {doc['filename']}")
            print(f"   URL: {doc['url'][:80]}...")
            print()

        # –°–ø—Ä–∞—à–∏–≤–∞–µ–º, —Å–∫–∞—á–∏–≤–∞—Ç—å –ª–∏
        response = input("–°–∫–∞—á–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã? (y/n): ")
        if response.lower() == 'y':
            result = downloader.download_documents(
                f"https://zakupki.gov.ru{test_tender_url}",
                test_tender_number
            )

            print("\n" + "="*70)
            print("  –†–ï–ó–£–õ–¨–¢–ê–¢ –ó–ê–ì–†–£–ó–ö–ò")
            print("="*70 + "\n")
            print(f"üìÅ –ü–∞–ø–∫–∞: {result['tender_dir']}")
            print(f"üì• –°–∫–∞—á–∞–Ω–æ: {result['downloaded']} –∏–∑ {result['total_documents']}")
            print(f"‚ùå –û—à–∏–±–æ–∫: {result['failed']}")
    else:
        print("\n‚ùå –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ URL —Ç–µ–Ω–¥–µ—Ä–∞.")

    print("\n" + "="*70 + "\n")


if __name__ == "__main__":
    main()
